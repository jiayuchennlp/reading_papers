A Survey of Large Language Models
## Abstract
-  four major aspects of LLMs
    - pre-trainingg (how to pretrain a capable LLM)
    - adaptation tuning(how to effectively adapt pre-trained LLMs for better use)
    - utilization(how to use LLMs for solving various downstream tasks)
    - capacity evaluation (how to evaluate the abilities of LLMs and existing empirical findings).
 -  summarize the available resources for developing LLMs
 -  discuss the remaining issues for future directions

## Introdution
从技术上来说，语言模型（LM）是一种能推动机器智能的手段。语言模型旨在对单词序列生成的可能性进行建模，从而预测未来的词汇。语言模型的发展经历了以下**四个阶段**：
1. **Statistical language models (SLM).**： 基于统计学习的方法，例如基于Markov假设推测下一个词。利用固定窗口n作为上下文窗口的语言模型称为*n-gram LM*。 这些方法的缺点是数据稀疏及维数灾难。
2. **Neural language models (NLM)**： 利用神经网络（如RNN）来建模单词序列的概率，利用*distributed representation*来表示词，可以利用到更长的上下文信息。其中涌现的重要思想是，采用语言模型来学习更好的表征向量，代表工作如*word2vec*。
3. **Pre-trained language models (PLM)**： 早期，*ELMo*提出了基于双向循环神经网络（*biLSTM*）的模型结构。随着*Transformer*结构的出现，*Bert*在基于双向*Transformer*结构，结合多种预训练任务（*Mask and predict& next word prediction*）,在各项任务上都取得了很大进展。*pre-training and fine-tuning* 的框架成为该时期的主流范式。同时也涌现了许多其他的模型结构（如*GPT*，*T5*）。
4. **Large language models (LLM).**：研究发现，随着模型参数量和数据规模的提升，语言模型的能力及在下游任务的表现也随之提升，出现大规模的语言模型（如*175B-parameter GPT-3 and the 540B-parameter PaLM*）。在复杂任务上，大语言模型会突然出现令人惊讶的能力（*called emergent abilities*）。
  
为了系统性地介绍LLM，先总结**PLM** 与 **LLM** 的区别：
1. LLM可以在处理复杂任务中表现出惊喜的效果，但小规模的PLM做不到
2. LLM改变了开发和使用AI算法的方法，可以通过*prompting interface*来使用如*GPT-4*等大模型
3. LLM的发展不再严格区分研究和工程， LLM的训练需要在大规模数据和分布式平行训练中进行，研究者们也需要去处理非常多的工程问题

**LLM带来的影响**：
1. *Chatgpt*和*GPT-4*的出现激发了重新思考人工通用智能的可能性。OpenAI文章[*Planning for AGI and beyond*](https://openai.com/blog/planning-for-agi-and-beyond)
2. 可能作为通用语言任务求解器，新的研究范式逐渐转向如何利用LLM
3. 在信息检索领域，正在受到来自ChatGPT这种新型的信息搜索方式的挑战。
4. CV领域，研究*ChatGPT-like vision-language models*和多模态对话。
5. 应用程序领域，促使了ChatGPT和各种插件的结合

**LLM的遗留问题**：原理待探索
1. 大模型突然涌现的惊喜能力（*emergent abilities*）待解释， 更普遍的，缺乏对大模型优越能力的深入、详细的研究。
2. 研究者难以训练好的LLM， 对计算资源的巨大需求太过于昂贵。因此，LLM通常由工业界研发，具体细节难以公开。
3. 难以对齐大模型和人类的价值观，大模型有可能产生有毒、虚构或者有害的内容。
   
