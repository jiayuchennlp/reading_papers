A Survey of Large Language Models
## Abstract
-  four major aspects of LLMs
    - pre-training
    - adaptation tuning
    - utilization
    - capacity evaluation
 -  summarize the available resources for developing LLMs
 -  discuss the remaining issues for future directions

## Introdution
从技术上来说，语言模型（LM）是一种能推动机器智能的手段。语言模型旨在对单词序列生成的可能性进行建模，从而预测未来的词汇。语言模型的发展经历了以下四个阶段：
1. **Statistical language models (SLM).**： 基于统计学习的方法，例如基于Markov假设推测下一个词。利用固定窗口n作为上下文窗口的语言模型称为*n-gram LM*。 这些方法的缺点是数据稀疏及维数灾难。
2. **Neural language models (NLM)**： 利用神经网络（如RNN）来建模单词序列的概率，利用*distributed representation*来表示词，可以利用到更长的上下文信息。其中涌现的重要思想是，采用语言模型来学习更好的表征向量，代表工作如*word2vec*。
3. **Pre-trained language models (PLM)**： 早期，*ELMo*提出了基于双向循环神经网络（*biLSTM*）的模型结构。随着*Transformer*结构的出现，*Bert*在基于双向*Transformer*结构，结合多种预训练任务（*Mask and predict& next word prediction*）,在各项任务上都取得了很大进展。*pre-training and fine-tuning* 的框架成为该时期的主流范式。同时也涌现了许多其他的模型结构（如*GPT*，*T5*）。
4. **Large language models (LLM).**：研究发现，随着模型参数量和数据规模的提升，语言模型的能力及在下游任务的表现也随之提升，出现大规模的语言模型（如*175B-parameter GPT-3 and the 540B-parameter PaLM*）。在复杂任务上，大语言模型会突然出现令人惊讶的能力（*called emergent abilities*）。
  
为了系统性地介绍LLM，先总结**PLM** 与 **LLM** 的区别
