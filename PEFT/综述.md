[Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.15647)

1. 动机： 模型参数量在2019年的330M暴涨到2022年的175B，增加了将近500倍，微调大模型需要大量的资源。尽管大模型有一定的zero-shot能力，但应用在下游任务上表现仍不佳。“获取更多数据”仍是在具体任务上提高性能的最佳方法。
2. 现有技术的挑战：
  - 缺乏理论理解
  - PEFT与finetune之间的gap
3. 分类：
  - 分类依据： 是否引入新的参数，是否微调大模型的部分参数
    
<div align=center>
<img src=https://github.com/jiayuchennlp/reading_papers/blob/main/PEFT/pictures/survey1.png/>
</div>

  - 三大类
      - Addition-based
        - Adapter-like: 在transformer的子层中引入小的全连接网络， 代表工作如Adapters, AdaMix
        - Soft prompts: 通过修改输入文本（如加入任务定义）和在输入层插入一定数量的embedding（可训练）。缺点是难以优化，对插入的embedding的个数和形式较敏感。 代表工作如：Prompt Tuning、Prefix-Tuning
          - Prompt Tuning: 在embedding层插入可调节的矩阵
          - Prefix-Tuning：在模型的每一个隐藏层插入可调节的矩阵
          - Intrinsic Prompt Tuning (IPT)：训练autoencoder
        - 其他方法
          -  Ladder-Side Tuning (LST)：在预训练大模型旁边训练一个小的网络，side network
          -   (IA)3: 将qkv进行缩放，仅训练缩放矩阵
      - Selection-based： 选择性地微调部分结构：如最顶的层、biases、特定列
        - BitFit： 仅训练biases
        - DiffPruning: 每次仅微调参数中的一部分（sparse update）
        - Freeze and Reconfigure (FAR)： 仅调节一些列
        - FishMask： 根据fisher information 决定调节模型中的哪些参数，需要计算全部的梯度，效率不高
      - Reparametrization-based： 在low-rank subspaces中调整学习参数，如LoRA
        - Intrinsic SAID： 使用Fastfood transform来降维


## Adapter Tuning

## Prefix Tuning

## Prompt Tuning

## P-tuning

## LoRA
[link](https://github.com/jiayuchennlp/reading_papers/blob/main/PEFT/LoRA.md)
